# Test

The **Test** menu provides an environment where you can perform behind-the-scenes testing. It lets you evaluate how your AI agent responds to different user inputs, ensuring it behaves as expected before going live. You can simulate real conversations, assess accuracy, and continuously improve agent performance over time.

* [Manage test scenarios](manage-test-scenarios.md)\
  Create test cases that represent ideal user-to-agent interactions. You can define expected responses, set context (such as user ID, country, or language), and manage each test individually.
* [Run tests](run-tests.md)\
  Group multiple test conversations into a test set and run them all at once. View pass/fail results, schedule recurring runs, and monitor success rates over time.

Testing is essential for validating your AI agent and ensuring consistent, high-quality performance across different scenarios. Whether you're testing one conversation or running full sets of tests, this space gives you the control to validate and refine your AI agent with confidence.\\
